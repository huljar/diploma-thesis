%To accomplish the goals 

%This chapter will illuminate the details of how encryption, authentication, network coding, and ARQ management are implemented in the network
%interfaces.

%Recall that the architecture of a \gls{noc} consists of a router grid with each node connected to a local processing element through a network
%interface (cf. Figure \vref{fig:nocexample}). The 

Over the course of this chapter, the design of the communication protocol in all variants will be thorougly explained. To begin, fundamental
assumptions about the environment and basic building blocks of the protocol are presented. Since the protocol variants share many elements and differ
mostly in the authentication scheme, the simplest one (uncoded individual authentication) is used to give a step-by-step explanation of the full
protocol. Following this, the other variants are described by drawing upon the differences to uncoded individual authentication.

\section{Design Aspects}
\subsection{Unicast And Multicast}
In Section \ref{sec:assandsig}, it was already hinted that this thesis concerns itself with the unicast communication pattern. In this scenario, each
flit is sent to a single specified destination. In contast, multicast facilitates the existance of multiple receivers for a single flit.

While there has been notable work on \gls{noc} multicast protocols, especially in conjuction with network coding, this thesis builds directly upon
research in the unicast domain (cf. Section \ref{sec:ncfornoc}). Hence, multicast will not be considered for the implementation, experiments, and
evaluation performed here.

\subsection{Flit Structure}\label{subsec:flitstructure}
As mentioned in Section \ref{sec:flitsfun}, flits are the fundamental unit of transmission for this thesis. They contain several header fields and a
payload. Their structure differs slightly depending on whether or not network coding is employed. For network coded flits, one additional
header field is required (the global encoding vector) and the generation ID replaces the flit ID. This structure is depicted in Figure
\vref{fig:flitstructureuncoded} (uncoded) and Figure \ref{fig:flitstructurenetworkcoded} (network coded).
% TODO: table comparing the protocol variants, their properties (e.g. lane width)

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{flit-structure-uc}
    \caption[The structure of a flit without network coding]{The structure and data layout of a flit in an environment without network coding
    (uncoded). The fixed size of the header fields and the payload results in an invariant total size of 141 bit.}
    \label{fig:flitstructureuncoded}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{flit-structure-nc}
    \caption[The structure of a flit with network coding]{The structure and data layout of a flit in a network coded environment. The fixed size of
    the header fields and the payload results in an invariant total size of 149 bit.}
    \label{fig:flitstructurenetworkcoded}
\end{figure}

The header fields are required for routing the flits to their destinations and for correct processing by the receivers, while the payload carries the
transmitted information. Uncoded flits have the following fields:
\begin{itemize}
    \item \textbf{Burst.} The burst bit is used to indicate an uninterrupted stream of flits between the same sender and receiver. For example, when a
        larger packet is being broken down into multiple flits, the burst bit can be used to indicate the beginning of a flit stream, allowing the
        intermediate routers on the path to the receiver to adjust their routing behavior accordingly in order to minimize stream interruptions. In
        this thesis, however, the burst bit is unused. As there are plans to build upon this work and explore burst configurations, it is still
        included for documentation purposes.
    \item \textbf{Source X, Source Y.} Recalling that the topology of the \gls{noc} is a 2D mesh, the usage of 2D cartesian coordinates to uniquely
        identify nodes suggested itself. Here, zero-based X and Y indices indicate the column and row of a node, respectively. Each field is 4 bit long,
        resulting in at most $16 \cdot 16 = 256$ nodes being addressable. Hence, the maximum \gls{noc} size is \textit{16x16} as well, which is deemed
        sufficient\footnote{It is certainly possible to increase the field sizes to, e.g., 8 bit, allowing for up to $256 \cdot 256 = 65536$ addressable
        nodes, should the need arise. However, as this would increase the flit size, it is not implemented here.}. The source header fields describe
        the address of the flit's sender.
    \item \textbf{Destination X, Destination Y.} These fields also describe the address of a node in 2D cartesian coordinates. As opposed to the
        source fields, they indicate the intended destination of the flit.
    \item \textbf{Mode.} The mode field indicates what type of data the flit contains. It signals to the receiver whether the flit contains, e.g.,
        data, a \gls{mac}, or an \gls{arq}. There are a few more special modes that are explained in Section \ref{sec:theprotocol}. The
        field size is 4 bit, allowing for 16 different modes. Not all of them are used for this thesis, so it is possible to define additional modes
        in a future work if necessary.
    \item \textbf{Address.} This field may contain a 32 bit memory address. In case the flit payload needs to be read from or written to memory, it
        indicates the location of the data. Since this is only relevant to the processing elements and not the network interfaces, the field remains
        unused for this thesis. Nevertheless, it is included in the flits since it affects their size.
    \item \textbf{Flit ID.} The flit ID is a 24 bit numeric identifier. Starting at zero, the IDs are incremented as more flits are generated.
        Similar to sequence numbers (e.g., as employed in \gls{tcp}), they are used to ensure that all flits arrive at the destined processing
        elements in order. Furthermore, it is used to associate related flits. For instance, an encrypted data flit and the corresponding \gls{mac}
        flit have the same ID to express their correlation.
    \item \textbf{Payload.} The payload carries the actual data that are transmitted with a flit. Its size is 64 bit as this is a common plaintext
        size for block ciphers, such as the one used for this thesis, PRINCE \cite{borghoff12prince}.
\end{itemize}
\vspace{0.5\baselineskip}

When network coding is applied to a flit, the following fields are added:
\begin{itemize}
    \item \textbf{Generation ID.} Replacing the flit ID, the generation ID is also a 24 bit numeric idenfier. As network coding intermingles multiple
        flits, their original IDs become meaningless and are replaced by the generation ID (which is the same for all flits of a generation). Apart
        from this, they follow the same rules as the flit IDs described above.
    \item \textbf{Global Encoding Vector.} This field contains the global encoding vector used to create the flit when applying network coding. It is
        required for the receiver to successfully decode a generation and obtain the original flits. Furthermore, since all flits in a generation have
        the same ID, it is used to distinguish between those flits. As with flit IDs in an uncoded setting, corresponding flits (like data and
        \gls{mac}) are associated by having the same \gls{gev} in addition to the same generation ID.
\end{itemize}
\vspace{0.5\baselineskip}

With a size of 141 or 149 bit, respectively, flits are relatively small. While this entails that each one carries comparatively little
information\footnote{The combined size of the header fields (77 or 85 bit, respectively) exceeds the size of the payload (64 bit) considerably.}, it
has the significant advantage that flits can be transferred between directly connected components in a single clock
cycle. For example, sending a flit from a router to a neighboring one, or sending it from a processing element
to the attached network interface takes one cycle. In a hardware implementation, this is facilitated by parallel communication channels that are able
to transmit multiple bits simultaneously. This results in multiple wires (or other electical conductors) at the physical layer, as each one may convey
precisely one bit of information per cycle \cite{wikiparallelcomm}. Thus, for uncoded flits, a channel width of 141 or more wires is required, and at least 149
in a network coded setting.

\subsection{Cryptography}\label{subsec:crypto}
\subsubsection{Notation}\label{subsubsec:cryptonotation}
This section introduces formal notations and definitions regarding the cryptographic primitives. They will be used throughout this chapter to
illustrate how flits are processed. They are denoted by variables, functions, and operators, where
\begin{itemize}
    \item $p$ is a plaintext message (which corresponds to the flit payload),
    \item $c$ is a ciphertext,
    \item $m$ is a \gls{mac},
    \item $k$ is a key (with $k_e$ and $k_m$ referring to encryption and \gls{mac} keys),
    \item $f$ is a complete flit (header and payload),
    \item $r$ is an authenticated and encrypted message,
    \item $e_k(p)$ is an encryption function that takes a plaintext $p$ and produces a ciphertext $c$ using key $k$,
    \item $d_k(c)$ is a decryption function that takes a ciphertext $c$ and produces the plaintext $p$ using key $k$,
    \item $a_k(f)$ is an authentication function that takes a flit and produces a \gls{mac} $m$ using key $k$\footnote{The whole flit is
        authenticated, as opposed to just the payload, to provide integrity on the header fields as well.},
    \item $\oplus$ denotes the bitwise XOR operation, and
    \item $\|$ symbolizes concatenation.
\end{itemize}
\vspace{0.5\baselineskip}

While the functions $e_k(p)$ and $d_k(c)$ take a 64 bit long text as their argument (which matches the cipher's block size), the authentication
function $a_k(f)$ is applied to complete flits, exceeding the block size. Therefore, $a_k(f)$ applies the cipher in conjunction with the \gls{cbcmac}
mode of operation as explained below.

\subsubsection{Drawbacks}
The integration of cryptographic operations inevitably entails a degradation of the network performance which manifests itself in higher transmission
latencies and increased chip area requirements. There usually is a reversely proportional relation between performance and safety: as the level of security
increases, the communication performance declines. In order to satisfy the protection goals of confidentiality and integrity, both encryption and
authentication are required\footnote{Depending on the use case, it may be adequate to fulfill only one of these goals (or none at all), alleviating
the performance drop caused by the security measures. For instance, a preceding work of this thesis concerns itself with providing only integrity in
\glspl{noc} \cite{moriam18activeattackers}.}, resulting in a noticeable performance impact. In order to minimize these effects, several countermeasures
are implemented:
\begin{itemize}
    \item \textbf{Parallel modules.} As the cryptographic computations are the operations with the highest latency in the network interfaces, they
        threaten to become a bottleneck and congest the traffic flow. To prevent this, multiple cryptographic modules are employed in each network
        interface. While this does not speed up the individual cryptographic computations, it allows for multiple flits to be processed in parallel. For
        instance, each network interface could have three encryption modules, allowing it to encrypt up to three flits simultaneously. Alas, this
        approach naturally increases the occupied chip area.
    \item \textbf{Hardware-optimized ciphers.} In Section \ref{sec:lightweightcrypto}, it was already mentioned that conventional block ciphers, such
        as \gls{aes}, are not efficient enough for usage in \glspl{noc}, in terms of both latency and chip area. However, there is a plethora of
        algorithms specifically designed for an efficient hardware implementation. In the work directly preceding this thesis, they were evaluated and
        compared \cite{harttung17lightweightcrypto}. The most promising algorithm is PRINCE \cite{borghoff12prince}, which is introduced below.
    \item \textbf{Parallel decryption and verification.} When flits arrive from the network, the \gls{mac} is verified to ensure integrity. When
        successful, the data are decrypted and passed on. To speed up this process, verification and decryption is performed in parallel. Should the
        integrity check fail, the decrypted data have to be discarded. In case of success, however, this results in a significant latency reduction.
        This scheme is illustrated in detail in the step-by-step description of the protocol (Section \ref{sec:theprotocol}).
\end{itemize}

\subsubsection{Composition Methods}
The ordering of encryption and authentication (i.e., \gls{mac} computation) has a significant impact on both the achievable performance and the
security of the scheme. There are three general approaches to bring them together \cite{bellare00authenc}:
\begin{itemize}
    \item \textbf{Encrypt-and-MAC (\gls{eam}).} The plaintext is encrypted and the \gls{mac} is computed over the plaintext. Afterwards, the \gls{mac} is
        appended to the ciphertext. More precisely, $r = e_{k_e}(p)\|a_{k_m}(f)$.
    \item \textbf{MAC-then-Encrypt (\gls{mte}).} The \gls{mac} is computed over the plaintext and then appended to it. The result is then
        encrypted. In other words, $r = e_{k_e}(p\|a_{k_m}(f))$.
    \item \textbf{Encrypt-then-MAC (\gls{etm}).} First, the plaintext is encrypted. Subsequently, the \gls{mac} is computed over the ciphertext and appended to it.
        Namely, $c = e_{k_e}(p)$ and $r = c\|a_{k_m}(f_c)$ where $f_c$ refers to the flit with encrypted payload.
\end{itemize}
\vspace{0.5\baselineskip}

Out of these three methods, \gls{etm} was chosen. It provides integrity on the ciphertext -- and consequently on the plaintext as well --
instead of just the plaintext. Thus, the integrity of the message can be verified without the need to decrypt it first and the parallel decryption and
verification mentioned above becomes possible. Moreover, in case of an integrity breach, \glspl{arq} can be issued earliest as the verification is
performed directly after the flits arrive.

On the sender side, \gls{etm} necessitates sequential encryption and authentication since the \gls{mac} is computed over the ciphertext. While
\gls{mte} forces this as well, it can be parallelized with \gls{eam}. However, both \gls{eam} and \gls{mte} do not provide an overall speed gain over
\gls{etm}. In addition, they have been subject to security vulnerabilities in the past \cites{bellare00authenc}{bellare04ssheam}{etmtls}. Table
\ref{tab:compositionmethods} gives a comparison of the methods.

\begin{table}
    \centering
    \begin{tabulary}{\textwidth}{L|CCC}
        & \gls{eam} & \gls{mte} & \gls{etm} \\\hline
        Parallel encryption/authentication & yes & no & no \\
        Parallel decryption/verification & no & no & yes \\
        Plaintext integrity & yes & yes & yes \\
        Ciphertext integrity & no & no & yes \\ % TODO: more aspects?
        Verification latency & high & high & low
    \end{tabulary}
    \caption[Comparison of authenticated encryption composition methods]{Comparison of the authenticated encryption composition methods
    Encrypt-and-MAC (\gls{eam}, MAC-then-Encrypt (\gls{mte}), and Encrypt-then-MAC (\gls{etm}).}
    \label{tab:compositionmethods}
\end{table}

For all methods, the ciphertext and \gls{mac} need to be embedded into one or more flits since they cannot be sent without the header fields. This is
elucidated in the protocol descriptions (Section \ref{sec:theprotocol}).

\subsubsection{PRINCE}\label{subsubsec:prince}
The cipher that is used for encryption, decryption, and authentication is \textit{PRINCE} \cite{borghoff12prince}. Among its many competitors, it is
the algorithm of choice because it has the lowest encryption and decryption latency with a competitive area requirement
\cite{harttung17lightweightcrypto}. Here, low latency is deemed more important than low chip area requirements, rendering it a natural choice. Besides, the
faster the algorithm runs, the less parallel modules may be required, potentialls reducing the overall area requirement in the end. Furthermore, with a
key length of 128 bit, it provides relatively high security\footnote{Similar algorithms often have a key length of just 80 or 96 bit
\cite[5]{harttung17lightweightcrypto}.}. To date, it remains unbroken despite monetary incentives to develop attacks \cite{princechallenge}.

PRINCE is a symmetric block cipher with a block size of 64 bit and a key size of 128 bit. It is structured as a \textit{substitution-permutation
network (\gls{spn})} with 12 rounds.

What sets PRINCE apart from other lightweight ciphers is a design specifically targeted at fully unrolled hardware implementations. With this
strategy, each round is computed using its own dedicated part of the circuit. These pieces are concatenated to form the full cipher. This approach
enables the algorithm to be executed within a single clock cycle. In contast, traditional iterative implementations of round-based ciphers usually
have a single circuit constituting the round function that is reused for all rounds. There, one clock cycle per round is required.

Although single-cycle execution suggests a tremendously low latency, the number of cycles cannot be directly mapped to execution speed. The long critical path
entailed by the chained round functions results in a relatively low maximum clock frequency. Furthermore, the unrolled design entails a comparatively
higher area requirement than iterative ones; however, it is still \enquote{considerably lower than fully unrolled versions of \gls{aes} or PRESENT}
\cite[3]{borghoff12prince}.

The authors of PRINCE have investigated the maximum clock frequency of their algorithm under varying conditions. They were able to achieve 212.8 MHz
using speed-optimizing synthesis strategies and 45 nm technology\footnote{Their other tests include 90 and 130 nm technology or area-optimizing
synthesis strategies, yielding lower results.}. In other research at the TU Dresden related to this thesis, 28 nm technology is commonly used. Under
this premise, a frequency of at least 250 MHz is definitely achievable, considering that the switch from 90 to 45 nm already more than doubled
the maximum frequency in the authors' experiments.

A great asset of PRINCE is the possibility to use almost the exact same algorithm for encryption and decryption. More precisely, the decryption
function corresponds to the encryption function if the round key\footnote{PRINCE uses the same round key for all rounds.} for the former is slightly
modified. At the start of PRINCE, three subkeys $(k_0, k_0', k_1)$ are derived from the main key $k$, with $k_1$ being the round key\footnote{The
other keys, $k_0$ and $k_0'$ are used as whitening keys.}. Now, $d_{(k_0, k_0', k_1)}(c) = e_{(k_0, k_0', k_1 \oplus \alpha)}(c)$, where $\alpha$ is a
64 bit constant. This property, called \textit{\alpha-reflection}, renders using the same circuit for encryption and decryption possible by simply
omitting the XOR operation for the encryption case.

\subsubsection{A Note On Stream Ciphers}
While block ciphers dominate the lightweight cryptography field of research, there has been notable research on stream ciphers as well
\cite[cf.][]{estream}. In terms of speed and size, they sometimes even surpass block ciphers. For example, the cipher \textit{Trivium}
\cite{decanniere06trivium} is smaller than PRINCE and also able to encrypt up to 64 bit in one clock cycle \cite[8]{harttung17lightweightcrypto}.

The reason stream ciphers were ultimately not considered for this thesis is twofold. First, they require keeping an internal state from which the
keystream is derived (288 bit for Trivium). As each node pair uses its own shared secret key, all nodes would need to keep such state information for
every other node in the network, necessitating large buffers. Second, in comparison to block ciphers, stream ciphers have been studied very little,
potentially increasing the risk of security flaws in this construction technique.

\subsection{Random Linear Network Coding}\label{sec:designnc}
The coding scheme used in this work is a variant of \textit{Random Linear Network Coding (\gls{rlnc})} and based on the work of
\citeauthor{moriam15manycorenc} \cite{moriam15manycorenc}. It slightly differs from the traditional idea of network coding
\cites{ahlswede00networkflow}{li03linearnc}. First, the targeted communication pattern is not multicast, but unicast transmissions, since only the
latter are of interest for this thesis. Second, only the sender nodes compute combinations of flits (intermediate nodes merely forward them) and only
flits with the same destination are coalesced (\enquote{intra-session network coding} \cite[1]{moriam15manycorenc}). The rationale behind this are the
tight performance requirements of the \gls{noc}. If each network node were to buffer incoming flits until enough are present to perform local
encoding, routing latencies would increase considerably. Furthermore, the implementation of local encoding necessitates additional memory and
processing logic inside each router, making their design more complex and increasing their size. As this directly contradicts the \gls{noc} design goals
laid out in Section \ref{sec:networkonchipfun}, it is not considered here.

\subsubsection{Operation Breakdown}
In the implemented approach, the sender organizes flits $f_i\ |\ i \in \mathbb{N}, i = 0, …, n$ with the same destination into generations $g_j\ |\ j
\in \mathbb{N}, j = 0, …, \lfloor\frac{n}{2}\rfloor$ of size $G$. Here, each generation consists of 2 flits (i.e., $G = 2$)\footnote{Larger
generations, consisting of three or more flits, are also feasible, but have proven to entail higher transmission latencies \cite[2]{moriam18activeattackers}, which
is not desirable.} and is constructed as $g_j = (f_{2j}, f_{2j+1})$. Furthermore, the flit IDs $i$ are replaced in the flits by the generation IDs $j$
as described in Section \ref{subsec:flitstructure} (see also Figure \vref{fig:encodingexample}).

The combinations are then computed as follows: first, a unit vector $e_i = (e_{i_0}, …, e_{i_{G-1}})$ that signifies the index of the flit inside the
generation \cite[3]{chou03practicalnc} is prepended to each flit's payload:
\[
    e_{i_k} =
    \begin{cases}
        1 & |\ i = k \pmod G \\
        0 & |\ i \neq k \pmod G
    \end{cases}
\]
These vectors and the payloads are then organized as a matrix for each generation:
\[
    \begin{split}
        \begin{bmatrix}
            e_{0,\,0} & \cdots & e_{0,\,G-1} & p_{0,\,0} & \cdots & p_{0,\,P-1} \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            e_{G-1,\,0} & \cdots & e_{G-1,\,G-1} & p_{G-1,\,0} & \cdots & p_{G-1,\,P-1}
        \end{bmatrix}\\
        =
        \begin{bmatrix}
            1 & & 0 & p_{0,\,0} & \cdots & p_{0,\,P-1} \\
            & \ddots & & \vdots & \ddots & \vdots \\
            0 & & 1 & p_{G-1,\,0} & \cdots & p_{G-1,\,P-1}
        \end{bmatrix}
    \end{split}
\]
where $P$ corresponds to the payload size in symbols. Now, $C \geq G$ combinations are computed with randomly selected coefficients $\beta$
\cite[cf.][]{ho03randomcoding}:
\[
    \begin{split}
        \begin{bmatrix}
            \beta_{0,\,0} & \cdots & \beta_{0,\,G-1} & q_{0,\,0} & \cdots & q_{0,\,P-1} \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \beta_{C-1,\,0} & \cdots & \beta_{C-1,\,G-1} & q_{C-1,\,0} & \cdots & q_{C-1,\,P-1}
        \end{bmatrix}\\
        =
        \begin{bmatrix}
            \beta_{0,\,0} & \cdots & \beta_{0,\,G-1}\\
            \vdots & \ddots & \vdots \\
            \beta_{C-1,\,0} & \cdots & \beta_{C-1,\,G-1}
        \end{bmatrix}
        \begin{bmatrix}
            1 & & 0 & p_{0,\,0} & \cdots & p_{0,\,P-1} \\
            & \ddots & & \vdots & \ddots & \vdots \\
            0 & & 1 & p_{G-1,\,0} & \cdots & p_{G-1,\,P-1}
        \end{bmatrix}
    \end{split}
\]
Each row of the result matrix represents one combination, with $\beta_i = (\beta_{i,\,0}, …, \beta_{i,\,G-1})\ |\ i \in \left[0, C\right)$ standing
for the \glspl{gev} and $q_i = (q_{i,\,0}, …, q_{i,\,P-1})\ |\ i \in \left[0, C\right)$ corresponding to the encoded payloads. The other header fields
are adopted unaltered from the original flits\footnote{The header fields of the original flits in the same generation contain the same values by
design, hence these values can simply be copied to the encoded flits.}. Figure \vref{fig:encodingexample} illustrates the whole process.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{encoding-example-mapping}
    \caption[Application of network coding to flits]{Application of network coding to flits. The generation size is G=2 and the number of computed
    combinations is C=3 (i.e., the G2C3 scheme). Using randomly selected cofficients $\beta$ as \glspl{gev}, the flits $f_0$ and $f_1$ are encoded,
    creating the combinations $c_0$, $c_1$, and $c_2$.}
    \label{fig:encodingexample}
\end{figure}

For this thesis, variants with $C = 3$ and $C = 4$ combinations per generation are investigated \cite[cf.][2]{moriam18activeattackers}. These schemes
are referred to as \textit{G2C3} and \textit{G2C4}, respectively.

On the side of the receiver, the combinations are decoded to recover the original flits. If at least $G$ out of the $C$
combinations arrive at their destination\footnote{In fact, $G$ linearly independent combinations are required. Fortunately, they fulfill this
condition with very high probability since the coefficients are randomized \cite[3]{chou03practicalnc}.}, the receiver is able to decode them by solving
the system of linear equations established by the received flits' \glspl{gev} and payloads
\[
    \begin{bmatrix}
        \beta_{0,\,0} & \cdots & \beta_{0,\,G-1} & q_{0,\,0} & \cdots & q_{0,\,P-1} \\
        \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        \beta_{C-1,\,0} & \cdots & \beta_{C-1,\,G-1} & q_{C-1,\,0} & \cdots & q_{C-1,\,P-1}
    \end{bmatrix}
\]
using Gaussian elimination until the matrix is in reduced row echelon form with the following structure:
\[
    \begin{bmatrix}
        1 & & 0 & p_{0,\,0} & \cdots & p_{0,\,P-1} \\
        & \ddots & & \vdots & \ddots & \vdots \\
        0 & & 1 & p_{G-1,\,0} & \cdots & p_{G-1,\,P-1}
    \end{bmatrix}
\]
Now, the original flits may be recreated. The payloads are the vectors $p_i = (p_{i,\,0}, …, p_{i,\,P-1})\ |\ i \in \left[0, G\right)$ obtained from
the system of linear equations above. The flit IDs are reconstructed from the generation ID $j$: with $G = 2$, they are $2j$ and $2j+1$. The other
header fields are simply copied over from one of the combinations as they have not changed during the encoding phase.

\subsubsection{Effect On The Network}
On the one hand, \gls{rlnc} provides the great benefit that packet loss is tolerable to a certain degree. If $C > G$, i.e., the number of computed
combinations per generation is greater than the generation size, then flawless decoding is still possible for the receiver when up to $C - G$ flits
are lost in transit.

On the other hand, the amount of flits injected into the network increases considerably depending on the coding scheme. For G2C3, 50\% more flits are
sent out, and 100\% more for G2C4 (compared to uncoded communication). However, the additional resilience to flit loss entails less \glspl{arq} to be
issued (in an unreliable network), which in turn may reduce the overall load.

\section{The Protocol}\label{sec:theprotocol}
As mentioned in Section \ref{sec:protocolvariants}, three different communication schemes were envisioned, with the first two subdivided into uncoded
and network coded versions. The remainder of this section provides a comprehensive specification of all variants. The steps described are all
performed in the network interfaces.

\subsection{Individual Authentication}
\subsubsection{Uncoded Variant}


For each sender-receiver pair, a
flit's ID must be unique when it is generated by the sending processing element\footnote{As the maximum possible value of a 24 bit number is
$2^{24}-1$, long-running systems are bound to cause an arithmetic overflow at some point. If this happens, IDs are allowed to repeat themselves,
restarting at zero.}. 

- authentication mode of operation ($a_k(f)$)
% uncoded (full explanation), network coded

\subsubsection{Network Coded Variant}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{protocol-chart-indauth-nc}
    \caption[short]{long}
    \label{fig:protchartindauthnc}
\end{figure}
% TODO: asterisk on coding steps, double asterisk on authenticate step, legend that says how many flits are needed and are output at these steps

\subsection{Interwoven Authentication}

\subsection{Full-Generation Authentication}

\section{Routing Strategies}
Mention the perceived/envisioned advantages of adaptive routing (different paths for each flit within the same sender-receiver pair → thanks
to NC enough flits will arrive hopefully even if one path contains a corrupted router).

\subsection{Deterministic XY}
The term \enquote{XY routing} stems directly from the behavior of the routers: flits are only routed along the X axis until they arrive in the column
that the receiver lies in, at which they are routed along the Y axis until reaching the receiver node.
% TODO: figure with a 2D mesh, X and Y arrows on the sides, example path marked for a transmission
\subsection{Dynamic Smart Random}
\subsection{Randomized Oblivious Multi-Phase Minimal}
% Both with XY phases and DSR phases
This strategy essentially splits the routing problem into two phases: find a path from sender to proxy node and from proxy to receiver. For each
phase, one of the aforementioned strategies can be applied as is.

\begin{itemize}
    \item Routing Strategies
        \begin{itemize}
            \item Mention routing is general, we are special case of 2D mesh with uniform edge weights → much simpler than arbitrary networks
            \item XY/YX
                \begin{itemize}
                    \item Deterministic path
                    \item Attacker controlling a single router can reliably disrupt communication between certain nodes
                    \item does not distribute flits of a generation across different paths
                \end{itemize}
            \item XY/YX + Valiant
                \begin{itemize}
                    \item Deterministic path only if fixed valiant
                \end{itemize}
            \item Random XorY
            \item Random XorY + Valiant
            \item When writing about this: MANHATTAN DISTANCE (same for XY, YX, random XY, ROMM)
            \item Terminology:
                \begin{itemize}
                    \item Static=deterministic vs. dynamic (one fixed source-destination route or not)
                    \item Oblivious vs. adaptive (take into account network state or not)
                    \item Source routing: source decides a path, this is inserted into flit as metadata
                    \item ROMM is oblivious, use own term RAMM (randomized adaptive multi-phase minimal) for other variant
                    \item ROMM: oblivious, dynamic
                    \item DOR (dimension order routing): oblivious, deterministic
                \end{itemize}
        \end{itemize}
\end{itemize}

\iffalse
\section{Notes}
\begin{itemize}
    \item Encryption/authentication ordering
        \begin{itemize}
            \item Encrypt-then-MAC: best practice. Sequential encrypt/authenticate on sender side, but parallel decrypt/verify
                on receiver side. Advantage: MAC can be computed on receiver side immediately when ciphertext arrives, even when
                MAC flit has not arrived yet (if ARQ is necessary, it can be issued right away)
            \item MAC-then-encrypt: bad. Sequential authenticate/encrypt on sender side and sequential decrypt/verify on receiver
                side.
            \item Encrypt-and-MAC: okay. Parallel encrypt/authenticate on sender side, but sequential decrypt/verify on receiver
                side (overall same latency as Encrypt-then-MAC, but without advantage of fast ARQs)
        \end{itemize}
    \item Uncoded transmission
        \begin{itemize}
            \item no network coding
            \item 2 methods: 1 data flit + 1 MAC flit OR 2 data/MAC split flits
        \end{itemize}
    \item Flit structure
        \begin{itemize}
            \item burst bit, source/target address, mode, address, GID/FID, GEV, payload
            \item mode: define if data/mac/split/arq
        \end{itemize}
    \item Network coded transmission
        \begin{itemize}
            \item Number of flits: G2C3 or G2C4
            \item 3 methods: 1 data flit + 1 MAC flit OR 1 MAC flit per generation OR 2 data/MAC split flits
            \item mention that coded clits are slightly larger due to GEV being embedded → requires wider lanes
        \end{itemize}
    \item ARQs
        \begin{itemize}
            \item Limited number of ARQs per transmission unit (UC: data/MAC pair or split pair, NC: generation)
            \item Timeout of x (e.g. 8) cycles until first ARQ is sent
            \item If limit >1: start larger timeout (→ round-trip of ARQ)
            \item Many different cases, insert some flow diagrams here
            \item The higher the ARQ timeout/limit, the less likely the flit is still in retransmission buffer
            \item → ARQ timeout/limit and retransmission buffer size have to correlate
            \item In the case that we only have 1 ARQ left that we are allowed to send: Wait for any ongoing MAC verifications
                so in case they fail, the flits can be included in the ARQ
        \end{itemize}
\end{itemize}
\fi
